{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30746,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model construct Notebook (Colab)\n",
        "\n",
        "**Contexte, principe et architecture proposé :**\n",
        "\n",
        "* Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task (https://arxiv.org/abs/2210.13382) --> finalement, image-captionning !\n",
        "* Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model (https://arxiv.org/abs/2401.09417)\n",
        "* Bi-Mamba+: Bidirectional Mamba for Time Series Forecasting (https://arxiv.org/abs/2404.15772)\n",
        "* MambaByte : MambaByte: Token-free Selective State Space Model (https://arxiv.org/abs/2401.13660)\n",
        "\n",
        "L'idée est que le modele genere du texte et des pixels sous formes de sequences, lors de la génération d'image, il y aura toujours des sauts de lignes (ASCII OA) lors du démarrage de l'image, mais egalement qu'on atteint la limite de l'image. Si les sequences généré à la suite n'ont pas la meme tailles, ca génére 2 images differentes. La complexité ici est que mamba doit assimiler la tache de \"copie\" et que l'interpreteur construise logiquement aussi bien les images que le texte. (un peu comme l'art ASCII, mais en ++) --> utiliser des couches d'attention ? à voir si la bidirectionnalité permet d'améliorer la copie"
      ],
      "metadata": {
        "id": "mMvZ6vLRkCaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code inspiration :**\n",
        "\n",
        "* https://github.com/hustvl/Vim\n",
        "* https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py\n",
        "* https://huggingface.co/JunxiongWang/MambaByte_Arxiv\n"
      ],
      "metadata": {
        "id": "jty3MLefkCaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q mamba-ssm causal-conv1d"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-09T19:05:13.742794Z",
          "iopub.execute_input": "2024-08-09T19:05:13.743587Z",
          "iopub.status.idle": "2024-08-09T19:05:26.521327Z",
          "shell.execute_reply.started": "2024-08-09T19:05:13.743542Z",
          "shell.execute_reply": "2024-08-09T19:05:26.519927Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzgUBfnYkCaY",
        "outputId": "a34abbca-04c1-487a-d36b-8f4d51c98c5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/85.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for mamba-ssm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for causal-conv1d (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, math\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from mamba_ssm.modules.mamba_simple import Mamba"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-09T19:05:26.523141Z",
          "iopub.execute_input": "2024-08-09T19:05:26.523547Z",
          "iopub.status.idle": "2024-08-09T19:05:26.528931Z",
          "shell.execute_reply.started": "2024-08-09T19:05:26.523509Z",
          "shell.execute_reply": "2024-08-09T19:05:26.527959Z"
        },
        "trusted": true,
        "id": "ryW68sZakCaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class MambaConfig:\n",
        "    dim: int # The input dimension of the input tensor.\n",
        "    d_state: int = 16 #16 # The dimension of the state space model.\n",
        "    d_conv : int = 4 # The convolutionnal windows\n",
        "    expand: int = 2 # E in paper/comments\n",
        "    depth : int = 8 # The number of residual S6 layers"
      ],
      "metadata": {
        "id": "AXv36oHBvPPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model, d_state, n_layers = 64, 16, 8\n",
        "config = MambaConfig(dim=d_model, d_state=d_state, depth=n_layers)"
      ],
      "metadata": {
        "id": "E2coz0KuzIzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch, length = 2, 64\n",
        "x = torch.randn(batch, length, d_model).to(\"cuda\")\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOHHpwCmtICS",
        "outputId": "212e391a-61b6-4d69-ac2c-566b0bb33ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 64, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BysMamba(nn.Module):\n",
        "    def __init__(self, config: MambaConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        # text & image(t) embedding\n",
        "        self.vocab_size = 256+512 # ASCII bytes + RGB 8*8*8 pixel\n",
        "        self.linear_embedding = nn.Embedding(self.vocab_size, config.dim)\n",
        "        self.patch_embedding = nn.Conv2d(1, self.vocab_size, kernel_size=patch_size, stride=stride) # 3D in future\n",
        "        # mamba part\n",
        "        self.in_mamba = Mamba(d_model=config.dim, d_state=config.d_state, d_conv=config.d_conv, expand=config.expand,)\n",
        "        self.layers = nn.ModuleList([Mamba(d_model=config.dim, d_state=config.d_state, d_conv=config.d_conv, expand=config.expand,) for _ in range(config.depth)])\n",
        "        self.out_mamba = Mamba(d_model=config.dim, d_state=config.d_state, d_conv=config.d_conv, expand=config.expand,)\n",
        "        # output\n",
        "        self.lm_head = nn.Linear(config.dim, self.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # shape : x : (B, M, N, L)\n",
        "        _,M,N,_ = x.shape\n",
        "        # embedding\n",
        "        xl = x[:, M//2, N//2, :] # img center\n",
        "        xl = self.linear_embedding(xl) # (B,L,D)\n",
        "        xp = self.patch_embedding(x).flatten(2).transpose(1, 2) # (B,L,D)\n",
        "        x = xl + xp\n",
        "        # bidirectional mamba input\n",
        "        x += self.in_mamba(x) + self.in_mamba(torch.flip(x, dims=[1])).flip([1])\n",
        "        # mamba intermediate layers\n",
        "        for layer in self.layers:\n",
        "            x += layer(x)\n",
        "        # bidirectional mamba output\n",
        "        x += self.out_mamba(x) + self.out_mamba(torch.flip(x, dims=[1])).flip([1])\n",
        "        # prediction output\n",
        "        x = self.lm_head(x) # probability\n",
        "        return x"
      ],
      "metadata": {
        "id": "CpfefxVonGZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "# bytes([i for i in range(16**3)].decode('utf-8') # doesn't work, max 256\n",
        "s = [chr(i) for i in range(16**3)]\n",
        "text_byte = np.frombuffer(\"\".join(s).encode('utf-8'), dtype=np.uint8)\n",
        "# validation\n",
        "embedding = nn.Embedding(256+512, d_model).to(\"cuda\") # 256 for ASCII text and 512 for image"
      ],
      "metadata": {
        "id": "opwKIYER2pZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text = \"\\documentclass[12pt]{article}\"\n",
        "#text_byte = np.frombuffer(text.encode('utf-8'), dtype=np.uint8)\n",
        "input_ids = torch.from_numpy(text_byte[None, :]).long().cuda()\n",
        "input_ids.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDqNwUiXy2Yp",
        "outputId": "23791ef6-8bf5-4264-b914-a5280300fb14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10112])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_ = embedding(input_ids)\n",
        "x_.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PtaAuPV734n",
        "outputId": "e0415f60-f3b4-4047-aad1-01ec9032c72b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10112, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BysMamba(config).to(\"cuda\")\n",
        "y = model(x_)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx3WX91F1gWi",
        "outputId": "76a17d36-f70a-42f2-ef1c-103db0dbd580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10112, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    }
  ]
}